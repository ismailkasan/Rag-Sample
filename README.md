# ğŸ§  RAG.NetCoreSample

This project is a simple **Retrieval-Augmented Generation (RAG)** system built with .NET Core. When a user submits a question, relevant content is first retrieved from local documents, then used as context for a large language model (LLM) to generate a meaningful response.

## ğŸš€ Features
- .NET Core Web API architecture
- Support for OpenAI (GPT-4) or **local Ollama** models
- Simple `in-memory` vector store
- Embedding (text-to-vector conversion) and similarity search
- Easily extensible service-based structure

---

## ğŸ§± Project Structure

```
ğŸ“ RAG.NetCoreSample
â”œâ”€â”€ Controllers/AskController.cs          // Handles user questions and returns answers
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ EmbeddingService.cs               // Embedding logic (OpenAI)
â”‚   â”œâ”€â”€ GptService.cs                     // GPT answer generation (OpenAI)
â”‚   â””â”€â”€ VectorSearchService.cs           // Similarity search
â”œâ”€â”€ Data/InMemoryVectorStore.cs          // Vector storage and retrieval
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ AskRequest.cs
â”‚   â””â”€â”€ AskResponse.cs
â”œâ”€â”€ Program.cs                            // Dependency injection and app setup
â”œâ”€â”€ appsettings.json                      // API configuration
â””â”€â”€ README.md                             // This file
```

---

## âš™ï¸ Setup

### 1. Requirements
- [.NET 7+ SDK](https://dotnet.microsoft.com/download)
- [Ollama](https://ollama.com) (optional, for local LLM)
- OpenAI API Key (for embedding and GPT)

### 2. Add OpenAI Key
In your `appsettings.json`:
```json
{
  "OpenAI": {
    "ApiKey": "sk-xxxx"
  }
}
```

### 3. Run the App
```bash
dotnet build
dotnet run
```

---

## ğŸ” Example Usage

### POST `/api/ask`
```json
{
  "question": "How is provisioning done for Biotekno products?"
}
```

### Response:
```json
{
  "answer": "To perform provisioning for Biotekno products..."
}
```

---

## ğŸ§ª Add Test Data

At startup, you can load some sample embeddings:

```csharp
var chunks = TextSplitter.SplitIntoChunks(File.ReadAllText("docs.txt"));
foreach (var chunk in chunks)
{
    var emb = await embeddingService.GetEmbeddingAsync(chunk);
    vectorStore.AddChunk(chunk, emb);
}
```

---

## ğŸ”„ Using with Ollama (Local LLM)

1. Install Ollama:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

2. Pull a model:
```bash
ollama run mistral
```

3. Replace `GptService` with your own `OllamaService`, and call the `/api/generate` endpoint using HTTP POST with your prompt.

---

## ğŸ“Œ Notes

- `EmbeddingService` currently uses OpenAI. You can integrate local embeddings via `.NET + Python` bridge.
- `InMemoryVectorStore` is for demo use only. In production, consider using FAISS, Qdrant, or similar vector DBs.

---

## ğŸ“¬ Contribution & License
This project is open-source under the MIT License. Feel free to fork and contribute via pull requests.

---

**Author:** ismail KaÅŸan / 2025 âœ¨